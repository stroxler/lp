---
title: "Demos from O'Reilly's Learning R (Richard Cotton)"
author: "Steven Troxler"
date: "December 1, 2015"
output: html_document
---

```{r, message=FALSE}
library(ggplot2)
library(dplyr)
```

The later chapters of *Learning R* have some cool demos of working with data. Here
I try to capture some of the most interesting practical examples which aren't in other
tutorials I've worked through.

Most of the demos here involve input/output, although at the bottom there's a short
blurb on plotting.

# Reading and writing data

Almost all R tutorials discuss `read.table`, `write.table`, `read.csv`, and `write.csv` for
delimited tables. Here we discuss some of the other forms of data IO covered by *Learning R*.

## Plain text

Use `readLines` to read plain-text files that can fit in memory; this is akin to `python`s
`with open(path, 'r') as f: f.readlines()`:
```{r}
setwd("~/lp/R/learningr/")
plain_text_path <- system.file("extdata",
                               "Shakespeare's The Tempest, from Project Gutenberg pg2235.txt",
                               package="learningr")
my_text <- readLines("a_plain_text_file.txt")
my_text
```

We can write using `writeLines`, which prints to stdout by default but writes
in "w" mode if you give it a path
```{r}
writeLines(my_text, "another_text_file.txt")
```

To write a single large string to a file, use `cat()`, which like `writeLines`
defaults to `stdout` but optionally accepts a path as its second argument.

## XML

When you read XML using the `XML` library, you get back documents with a
special type. If you add `useInternalNodes = FALSE` to the call, you'll be
able to use `str` and `head` to view the document. That said, the output is
extremely verbose, so we suppress the output here
```{r, message=FALSE}
xml_path <- system.file("extdata", "options.xml", package="learningr")
r_options <- XML::xmlParse(xml_path)
# can't use str(r_options) / head(r_options)
r_options <- XML::xmlParse(xml_path, useInternalNodes = FALSE)
# we comment this out because it's too verbose, but this will work:
# head(r_options)
```

There is also an `xmlTreeParse` function, which is like `xmlParse` with
`useInternalNodes = FALSE` by default.

You can work with XML using the XPath language, using the
`XML::xpathSApply` function. There's a lot of other xml functionality,
but most of it is hard to learn without a real problem to work on.

If you need to write R data to an xml file (for example so that another
tool or language can read it), the `XML` package won't help, but
`RUniversal::makexml` will: you can apply it to a list and it will
output a string; you can then use `cat` to write it to a file. (This,
the author points out, is how he made `options.xml` in the first place.)

## Json

There are two main packages for working with json, `rjson` and `RJSONIO`.
Since they are similar (they differ in how they handle NA / NaN), I'll just
demo `rjson`, which prefers to work with in-memory strings (`RJSONIO`, like
`XML`, likes to read directly from file).

To convert an R object to json, use `rjson::toJSON`:
```{r}
sample_data_frame <- data.frame(var1 = c(2, 3, 4), var1 = c("cat", "cat", "dog"))
json1 <- rjson::toJSON(
  list(x = c(1, 2, 4, NA), y = NaN, z = list(a = 5, b = "hello")))
json2 <- rjson::toJSON(sample_data_frame)
json1
```

Note how the data frame is interpreted as if it were a list of columns. This
is natural given that R treats data frames a bit like lists, but is not best
practice when working with other languages; we will come back to this.

To convert json back to R, use `rjson::fromJSON`:
```{r}
rjson::fromJSON(json1)
rjson::fromJSON(json2)
```

Note that to reconstruct our `NA` and `NaN` observations, we would need to
convert them explicitly. To reconstruct a data frame you dumped as json,
wrap the `fromJSON` call in an `as.data.frame`:
```{r}
as.data.frame(rjson::fromJSON(json2))
```

## Yaml

You can read yaml from a file using `yaml::load_file(file_path)`; you can
similarly load yaml strings. You can dump an R object to yaml using
`yaml::as.yaml`.

## Records

The way that R converts `data.frame`s to `list`s is natural given how R
works, but it is counterproductive for working with many other tools like
D3: these tools tend to expect tabular data to be formed as lists of maps.
In the `pydata` stack, the `DataFrame` class has methods `to_records` and
`from_records` to make this conversion easy.

Here, we define a `to_records` function for R. The `from_records` case is a
bit harder to write, and slightly less likely to be important so we omit it.

```{r}
to_records <- function(df) {
  records <- split(df, seq(nrow(df)))
  names(records) <- NULL
  records
}
rjson::toJSON(to_records(sample_data_frame))
```

## SQL 

We first open a connection. We will use the `sqldf` package to work with sqlite,
although a very similar api can be used for other sql databases.
```{r, message=FALSE}
library('sqldf')
```
```{r}
setwd('~/lp/R/learningr/')
system('bash make_demo_db.sh')
db <- dbConnect(SQLite(), dbname="example.sqlite")
```

We can then read a full table in one shot
```{r}
dbReadTable(db, 'mytable')
```

We can update the table using a query (there are probably better ways to do
this from data frames, but for this demo we'll stick with straight SQL)
```{r}
ignore = dbSendQuery(db, 'insert into mytable (label, value) values ("her", 15.3);')
```

And we can also fetch the results of a query which isn't a full table:
```{r}
queryResult = dbSendQuery(db, 'select * from mytable where value > 12;')
dbFetch(queryResult)
```

## Other important file formats

I won't go into examples now, but R has packages that make it easy
to read data from MS Excel files, HDF5 archives, matlab files, and SAS files.

Most other data sources (mongodb, redshift, etc) likely also have R packages;
above I just list the ones I know for certain.

# Plotting

I have already made examples of most of the plots from the plotting
chapter of *Learning R*, but I do want to go through the demonstration of
a bar chart with two independent variables.

Take a look at the mtcars dataset:
```{r}
tbl_df(mtcars)
```

We are going to demonstrate bar charts of the number of observations
versus the number of cylenders and carburetors. To form a stacked
bar chart, just set (in `aes`) `x` to the main variable you want to use,
and `fill` to the secondary variable:
```{r}
ggplot(mtcars, aes(x=factor(cyl), fill=factor(carb))) +
    geom_bar()
```

If you prefer a non-stacked barplot, you add `position = "dodge"` to the
`geom_bar` definition, which causes it to place the bars side-by-side
rather than stacked:
```{r}
ggplot(mtcars, aes(x=factor(cyl), fill=factor(carb))) +
    geom_bar(position="dodge")
```

A general note on bar plots that came up when I was searching for examples: by
default, the `aes` call only needs an `x`, in which case the `y` is implicitly the
count of the number of rows.

# Modeling

We will demo some of R's linear modelling abilities on the `gonorrhoea` dataset
from the `learningr` package:
```{r}
gonorrhoea <- tbl_df(learningr::gonorrhoea)
gonorrhoea
```

This dataset, in addition to having some quantitative variables, also has several
`factors`, or qualitative variables. Let's take a look...
The `Filter` builtin function filters the
columns of a data frame by a predicate (oddly, there's no easy equivalent in
`dplyr`; perhaps this reflects that R naturally likes operating on the columns
of data frames, so `dplyr` focuses on row operations), and `lapply` works on columns
when applied to data frames
```{r}
lapply(Filter(is.factor, gonorrhoea), levels)
```

Running a model: We can create a model and get a first look at it with the
`summary` function - which is actually an `s3` method and overloaded for many
different R objects:
```{r}
model1 <- lm(Rate ~ Year + Age.Group + Ethnicity + Gender, gonorrhoea)
summary(model1)
```

If you `plot()` a model, it will cycle through diagnostic plots. The following
function, `plot_model`, uses `layout` to make all these plots at once:
```{r}
plot_model <- function(mod) {
  plot_numbers <- 1:6
  layout(matrix(plot_numbers, ncol = 2, byrow = TRUE))
  plot(mod, plot_numbers)
}
plot_model(model1)
```

A detailed understanding of these diagnostics is beyond the scope of this
tutorial (look in a book on linear models), but note that Cook's distance can
be useful for finding outliers; in our data, rows 40, 41, and 160 have been
flagged as outliers.

Note that if you wanted to poke around the internals of a `model`, you could
run the `str` and `unclass` methods to see what's in it. They are fairly
big, with lots of metadata.

We'll demo one other diagnostic plot from the *Learning R* book, a plot of
residual versus fitted value:
```{r}
diagnostics <- data.frame(
  residuals = residuals(model1),
  fitted = fitted(model1)
)
ggplot(diagnostics) + aes(fitted, residuals) + geom_point() +
  geom_smooth(method = "loess")
```

From this plot, we can see that the model errors seem to follow a U-shaped
pattern with respect to the fitted values, suggesting that a linear model
may not be appropriate.

We'll leave off with a discussion of building additional models and
comparing models. Let's try removing `Year` from `model1` to get a simpler
model:
```{r}
model2 = update(model1, ~ . - Year)
summary(model2)
```

We can compare the two models using `F` statistics via the `anova` function
```{r}
anova(model1, model2)
```

These models happen to have a nested structure which allows ANOVA for comparison,
but in general the AIC and BIC are often used to compare models of varying
complexity. Both of them try to trade off goodness of fit versus complexity;
AIC penalizes complexity less than BIC. A higher value is better (both of them
are equal to log likelihood minus a complexity penalty)
```{r}
AIC(model1, model2)
BIC(model1, model2)
```

In this case the `AIC` and `BIC` measures both mildly favor `model1`, but the
`F` statistic doesn't show a significant difference. Since our measures disagree,
we might choose the more complex `model1` if we cared mainly about prediction
accuracy and the simpler `model2` if we cared mainly about interpretability; keep
in mind that in the presence of outliers and without more detailed investigation
of the data, these relults may not mean much.

There are many more things we can do with models, and the `glm` function provides
a huge addition of models with similar diagnostics to what `lm` has. But since this
tutorial is more about R basics than linear models, we will stop here.